{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import udf,col"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d2e5dcf7-c4ee-4431-9613-fba1886b9073","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/UIDAI_ENR_DETAIL_20170308.csv\")  \ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1730b497-3e68-40fb-82f7-6dc5386b14f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-------------+----------+------------+--------+------+---+-----------------+------------------+-------------------------+---------------------------------+\n|           Registrar|    Enrolment Agency|        State|  District|Sub District|Pin Code|Gender|Age|Aadhaar generated|Enrolment Rejected|Residents providing email|Residents providing mobile number|\n+--------------------+--------------------+-------------+----------+------------+--------+------+---+-----------------+------------------+-------------------------+---------------------------------+\n|      Allahabad Bank|A-Onerealtors Pvt...|Uttar Pradesh| Allahabad|        Meja|  212303|     F|  7|                1|                 0|                        0|                                1|\n|      Allahabad Bank|Asha Security Gua...|Uttar Pradesh| Sonbhadra| Robertsganj|  231213|     M|  8|                1|                 0|                        0|                                0|\n|      Allahabad Bank|   SGS INDIA PVT LTD|Uttar Pradesh| Sultanpur|   Sultanpur|  227812|     F| 13|                1|                 0|                        0|                                1|\n|      Allahabad Bank|Sri Ramraja Sarka...|Uttar Pradesh|    Shamli|      Shamli|  247775|     M|  6|                1|                 0|                        0|                                1|\n|      Allahabad Bank|  Transmoovers India|Uttar Pradesh| Gorakhpur|    Sahjanwa|  273001|     M|  8|                1|                 0|                        0|                                1|\n|      Allahabad Bank|  Transmoovers India|Uttar Pradesh|  Varanasi|      Pindra|  221101|     M| 14|                1|                 0|                        0|                                1|\n|      Allahabad Bank|  Transmoovers India|Uttar Pradesh|  Varanasi|    Varanasi|  221001|     M|  9|                1|                 0|                        0|                                1|\n|      Allahabad Bank|  Transmoovers India|Uttar Pradesh|  Varanasi|    Varanasi|  221002|     M|  4|                1|                 0|                        0|                                1|\n|      Allahabad Bank|  Transmoovers India|Uttar Pradesh|  Varanasi|    Varanasi|  221002|     M| 10|                0|                 1|                        0|                                1|\n|      Allahabad Bank|  Transmoovers India|Uttar Pradesh|  Varanasi|    Varanasi|  221002|     M| 19|                1|                 0|                        0|                                1|\n|      Allahabad Bank|Vedavaag Systems ...|Uttar Pradesh|Bara Banki|   Nawabganj|  225301|     M|  8|                1|                 0|                        0|                                0|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|        Assam|  Marigaon|   Bhuragaon|  782121|     M| 22|                1|                 0|                        0|                                1|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|        Bihar| Gopalganj|  Vijayeepur|  841508|     M| 26|                1|                 0|                        0|                                1|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|    Karnataka|  Bagalkot|      Badami|  587114|     M| 27|                1|                 0|                        0|                                1|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|    Karnataka|  Bagalkot|      Badami|  587155|     F|  2|                1|                 0|                        0|                                1|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|    Karnataka|  Bagalkot|      Badami|  587155|     M| 67|                1|                 0|                        0|                                1|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|    Karnataka|  Bagalkot|      Badami|  587201|     F| 32|                1|                 0|                        0|                                1|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|    Karnataka|  Bagalkot|      Badami|  587203|     M| 27|                1|                 0|                        0|                                1|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|    Karnataka|  Bagalkot|      Badami|  587206|     F| 40|                1|                 0|                        0|                                0|\n|Atalji Janasnehi ...|Atalji Janasnehi ...|    Karnataka|  Bagalkot|      Badami|  587206|     M| 28|                1|                 0|                        0|                                1|\n+--------------------+--------------------+-------------+----------+------------+--------+------+---+-----------------+------------------+-------------------------+---------------------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#a Create a dataframe with Total Aadhaar's generated for each state\n\n# Uisng filter function we can select the column and it records,grouping the records by states and counting only aadhar geneated values and arranging them in decscending orrder.\nimport pyspark.sql.functions as f\nAadhar_count = df.groupBy(\"State\").agg(f.sum(\"Aadhaar generated\").alias(\"Toatal_aadhar\"))\nAadhar_count.show(20,truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f6eb4b0e-0fff-436a-8e90-11a554720274","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------+\n|State            |Toatal_aadhar|\n+-----------------+-------------+\n|Nagaland         |545          |\n|Karnataka        |19764        |\n|Odisha           |18182        |\n|Kerala           |15143        |\n|Tamil Nadu       |32485        |\n|Chhattisgarh     |6604         |\n|Andhra Pradesh   |5798         |\n|Madhya Pradesh   |53276        |\n|Punjab           |6506         |\n|Manipur          |1323         |\n|Goa              |1167         |\n|Mizoram          |6279         |\n|Himachal Pradesh |1547         |\n|Puducherry       |83           |\n|Haryana          |6804         |\n|Jammu and Kashmir|1234         |\n|Jharkhand        |9868         |\n|Arunachal Pradesh|913          |\n|Gujarat          |34844        |\n|Delhi            |8426         |\n+-----------------+-------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#b.) Create a dataframe with Total Aadhaar's generated by each enrollment agency\nimport pyspark.sql.functions as f\nEnrollment_agencys = df.groupBy(\"Enrolment Agency\").agg(f.sum(\"Aadhaar generated\").alias(\"Total_aadhar\"))\nEnrollment_agencys.show(10,truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"061fd0d5-d4db-4128-8314-86bba518a76d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------------------------+------------+\n|Enrolment Agency                   |Total_aadhar|\n+-----------------------------------+------------+\n|APOnline Limited                   |305         |\n|Transmoovers India                 |5           |\n|Alankit Limited                    |3861        |\n|Twinstar Industries Ltd.           |4806        |\n|OSWAL COMPUTERS & CONSULTANTS      |872         |\n|SHRIKRISHNA KHANDASARI SUGAR M     |1           |\n|Atalji Janasnehi Directorate  GOK  |1369        |\n|Sri Ramraja Sarkar Lok Kalyan Trust|2642        |\n|Asha Security Guard Services       |768         |\n|CMS Computers Ltd                  |13126       |\n+-----------------------------------+------------+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#c.) Create dataframe with top 10 districts with maximum Aadhaar's generated for both Male and Female?\nimport pyspark.sql.functions as f\nMales = df.filter(\"Gender == 'M'\").groupBy(\"District\").agg(f.count(\"Aadhaar generated\").alias(\"Total_aadhar\")).sort(f.desc(\"Total_aadhar\"))\nprint('Top 10 districts with maximum Aadhaar generated for Male : ')\nMales.show(10,truncate=False)\n# Females\n# Uisng filter function we can select the column and it records,grouping the records by District and counting only aadhar geneated values and arranging them in decscending orrder.\nFemales=df.filter((\"Gender == 'F'\")).groupBy(\"District\").agg(f.count(\"Aadhaar generated\").alias(\"Total_aadhar\")).sort(f.desc(\"Total_aadhar\"))\nprint('Top 10 districts with maximum Aadhaar generated for Female : ')\nFemales.show(10,truncate=False)\n\n'''import pyspark.sql.functions as f\ntop_10_dist_max_df=df.filter(~(df.Gender == 'T')).groupBy(\"District\").sum(\"Aadhaar generated\").orderBy(f.desc(\"sum(Aadhaar generated)\"))\ntop_10_dist_max_df.show(10)'''\ntop_10_dist_max_df=df.filter((df.Gender != 'T')).groupBy(\"District\").sum(\"Aadhaar generated\").orderBy(f.desc(\"sum(Aadhaar generated)\"))\ntop_10_dist_max_df.show(10)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22f6bf9c-2a06-4b55-9953-74f2d6209916","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Top 10 districts with maximum Aadhaar generated for Male : \n+-----------------+------------+\n|District         |Total_aadhar|\n+-----------------+------------+\n|Barddhaman       |4276        |\n|North 24 Parganas|3772        |\n|South 24 Parganas|3630        |\n|Bhagalpur        |3543        |\n|Patna            |3485        |\n|Nadia            |3460        |\n|Murshidabad      |3018        |\n|Gaya             |2915        |\n|Kolkata          |2678        |\n|Katihar          |2622        |\n+-----------------+------------+\nonly showing top 10 rows\n\nTop 10 districts with maximum Aadhaar generated for Female : \n+-----------------+------------+\n|District         |Total_aadhar|\n+-----------------+------------+\n|North 24 Parganas|3121        |\n|Barddhaman       |2859        |\n|South 24 Parganas|2448        |\n|Patna            |1766        |\n|Bhagalpur        |1744        |\n|Nadia            |1673        |\n|Jalpaiguri       |1509        |\n|Gaya             |1487        |\n|Murshidabad      |1399        |\n|Howrah           |1393        |\n+-----------------+------------+\nonly showing top 10 rows\n\n+-----------------+----------------------+\n|         District|sum(Aadhaar generated)|\n+-----------------+----------------------+\n|South 24 Parganas|                 16207|\n|       Barddhaman|                 15821|\n|        Bhagalpur|                 14479|\n|North 24 Parganas|                 11272|\n|             Gaya|                 10755|\n|          Katihar|                  9479|\n|            Patna|                  8945|\n|      Murshidabad|                  8656|\n|       Samastipur|                  8230|\n|            Nadia|                  8162|\n+-----------------+----------------------+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#d.) Create a dataframe with Total Aadhaar's generated for top 10 least state\nimport pyspark.sql.functions as f\n# Uisng filter function we can select the column and it records,grouping the records by states and counting only aadhar geneated values and arranging them in decscending orrder.\nAadhar_count = df.filter(col(\"Aadhaar generated\") == 1).groupBy(\"State\").agg(f.count(\"Aadhaar generated\").alias(\"Toatal_aadhar\"))\nAadhar_count.show(10,truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"70eaa54b-d268-4530-9a50-2bb982a9e76d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------+-------------+\n|State         |Toatal_aadhar|\n+--------------+-------------+\n|Nagaland      |289          |\n|Karnataka     |12167        |\n|Odisha        |9105         |\n|Kerala        |9955         |\n|Tamil Nadu    |16464        |\n|Chhattisgarh  |3409         |\n|Andhra Pradesh|3519         |\n|Madhya Pradesh|27491        |\n|Punjab        |4944         |\n|Manipur       |334          |\n+--------------+-------------+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#e.) for which age most adhar card has declined?\n# Uisng filter function we can select the column and it records,grouping the records by states and counting only aadhar geneated values and arranging them in decscending orrder.\nimport pyspark.sql.functions as f\nAge = df.groupBy(\"Age\").sum(\"Enrolment Rejected\").sort(f.desc(\"sum(Enrolment Rejected)\"))\nAge.show(100,truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5edaa6c4-91d0-4c04-a252-63bc6eb2353f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----------------------+\n|Age|sum(Enrolment Rejected)|\n+---+-----------------------+\n|4  |5673                   |\n|3  |3842                   |\n|2  |3372                   |\n|1  |3333                   |\n|0  |3219                   |\n|5  |2208                   |\n|6  |1931                   |\n|7  |1572                   |\n|8  |1357                   |\n|9  |980                    |\n|10 |920                    |\n|11 |604                    |\n|12 |560                    |\n|13 |406                    |\n|18 |384                    |\n|14 |348                    |\n|22 |329                    |\n|20 |318                    |\n|21 |300                    |\n|25 |293                    |\n|15 |292                    |\n|19 |285                    |\n|23 |271                    |\n|16 |261                    |\n|24 |251                    |\n|27 |239                    |\n|28 |231                    |\n|17 |231                    |\n|26 |224                    |\n|30 |217                    |\n|29 |172                    |\n|35 |171                    |\n|32 |171                    |\n|31 |152                    |\n|34 |129                    |\n|60 |127                    |\n|45 |121                    |\n|40 |119                    |\n|33 |115                    |\n|62 |111                    |\n|65 |104                    |\n|70 |104                    |\n|41 |100                    |\n|36 |100                    |\n|67 |96                     |\n|37 |93                     |\n|38 |93                     |\n|39 |92                     |\n|42 |91                     |\n|51 |87                     |\n|50 |87                     |\n|47 |82                     |\n|55 |81                     |\n|52 |74                     |\n|48 |74                     |\n|46 |74                     |\n|57 |69                     |\n|61 |67                     |\n|44 |66                     |\n|43 |66                     |\n|58 |66                     |\n|66 |62                     |\n|72 |61                     |\n|56 |59                     |\n|54 |58                     |\n|64 |58                     |\n|71 |56                     |\n|68 |56                     |\n|59 |53                     |\n|49 |53                     |\n|63 |53                     |\n|77 |46                     |\n|53 |44                     |\n|69 |44                     |\n|73 |41                     |\n|75 |40                     |\n|74 |40                     |\n|80 |35                     |\n|76 |31                     |\n|78 |22                     |\n|79 |21                     |\n|82 |12                     |\n|84 |5                      |\n|86 |4                      |\n|88 |4                      |\n|87 |3                      |\n|83 |3                      |\n|85 |2                      |\n|81 |2                      |\n|92 |1                      |\n|89 |1                      |\n|100|1                      |\n|103|0                      |\n|91 |0                      |\n|94 |0                      |\n|97 |0                      |\n|93 |0                      |\n|95 |0                      |\n|98 |0                      |\n|90 |0                      |\n+---+-----------------------+\nonly showing top 100 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a3c8d4dc-5881-4a44-b16d-087e0bcfdd60","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d7002bbb-b3e1-494e-9f1b-1ac46a9025ee","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task_2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
