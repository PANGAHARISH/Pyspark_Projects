{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as f\nsimpleData = ((\"James\", \"Sales\", 3000),\n(\"Michael\", \"Sales\", 4600),\n(\"Robert\", \"Sales\", 4100),\n(\"Maria\", \"Finance\", 3000),\n(\"James\", \"Sales\", 3000),\n(\"Scott\", \"Finance\", 3300),\n(\"Jen\", \"Finance\", 3900),\n(\"Jeff\", \"Marketing\", 3000),\n(\"Kumar\", \"Marketing\", 2000),\n(\"Saif\", \"Sales\", 4100)\n)\nrdd1 = sc.parallelize(simpleData)\ndf = rdd1.toDF([\"employee_name\",\"department\",\"salary\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fb150780-6de5-46f2-aaf5-260b1a4de6b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\nres = df.withColumn(\"row_number\",row_number().over(windowSpec))\nres.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a2ac6b89-7e9d-480c-be3d-093c1d49c449","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         2|\n|       Robert|     Sales|  4100|         3|\n|         Saif|     Sales|  4100|         4|\n|      Michael|     Sales|  4600|         5|\n+-------------+----------+------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import rank\nres1 = df.withColumn(\"rank\",rank().over(windowSpec))\nres1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7d6ea574-efc7-44f6-b67f-6ee4467a65da","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import dense_rank\nres3=df.withColumn(\"dense_rank\",dense_rank().over(windowSpec))\nres3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9b6bbe77-eebe-492e-bd19-0072325c809c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lag    \nfrom pyspark.sql.functions import lag    \ndf.withColumn(\"lag\",lag(\"salary\",1).over(windowSpec)) \\\n      .show()  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5d8bda4f-fcb5-4b53-9675-40a58d5f8b22","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|3000|\n|          Jen|   Finance|  3900|3300|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|2000|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|3000|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|4100|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lead    \ndf.withColumn(\"lead\",lead(\"salary\",1).over(windowSpec)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b9c9e75f-8ee5-4aac-bc18-c4e92f242493","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|3300|\n|        Scott|   Finance|  3300|3900|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|3000|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|3000|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4100|\n|         Saif|     Sales|  4100|4600|\n|      Michael|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["windowSpecAgg  = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number \ndf.withColumn(\"row\",row_number().over(windowSpec)) \\\n  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1a321af4-f597-49f0-9b76-32bd0b0b8c71","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n|     Sales|3760.0|18800|3000|4600|\n+----------+------+-----+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#3. PySpark Window Analytic functions\n\n\n\n#3.1 lag Window Function\n\n\n\nfrom pyspark.sql.functions import lag,lead\n#windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n#df = df.withColumn(\"lag\",lag(\"salary\",1).over(windowSpec))\n#df.show()\n\n\n\n#3.2 lead Window Function\n#from pyspark.sql.functions import lead\nwindowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n#df = df.withColumn(\"lead\",lead(\"salary\",1).over(windowSpec))\ndf = df.withColumn(\"lag\",lag(\"salary\",1).over(windowSpec))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4aa90265-9a9e-4959-98a6-9bb38e147891","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|3000|\n|          Jen|   Finance|  3900|3300|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|2000|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|3000|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|4100|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["windowSpecAgg  = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number \ndf.withColumn(\"row\",row_number().over(windowSpec)).withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"98261b57-06b5-48f5-b04d-6ff41afa2140","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+---+-----+\n|employee_name|department|salary| lag|row|  sum|\n+-------------+----------+------+----+---+-----+\n|        Maria|   Finance|  3000|null|  1|10200|\n|        Scott|   Finance|  3300|3000|  2|10200|\n|          Jen|   Finance|  3900|3300|  3|10200|\n|        Kumar| Marketing|  2000|null|  1| 5000|\n|         Jeff| Marketing|  3000|2000|  2| 5000|\n|        James|     Sales|  3000|null|  1|18800|\n|        James|     Sales|  3000|3000|  2|18800|\n|       Robert|     Sales|  4100|3000|  3|18800|\n|         Saif|     Sales|  4100|4100|  4|18800|\n|      Michael|     Sales|  4600|4100|  5|18800|\n+-------------+----------+------+----+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_sum=df.withColumn(\"sal_sum\", sum(col(\"salary\")).over(windowSpecAgg))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"df1038d9-7bfc-4ac6-906f-1fac36895630","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["windowSpecAgg = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number\ndf.withColumn(\"row\",row_number().over(windowSpec)).withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a8dc4525-bee9-4a9c-a3fd-59dd4ac9b45d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+---+-----+\n|employee_name|department|salary| lag|row|  sum|\n+-------------+----------+------+----+---+-----+\n|        Maria|   Finance|  3000|null|  1|10200|\n|        Scott|   Finance|  3300|3000|  2|10200|\n|          Jen|   Finance|  3900|3300|  3|10200|\n|        Kumar| Marketing|  2000|null|  1| 5000|\n|         Jeff| Marketing|  3000|2000|  2| 5000|\n|        James|     Sales|  3000|null|  1|18800|\n|        James|     Sales|  3000|3000|  2|18800|\n|       Robert|     Sales|  4100|3000|  3|18800|\n|         Saif|     Sales|  4100|4100|  4|18800|\n|      Michael|     Sales|  4600|4100|  5|18800|\n+-------------+----------+------+----+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nsimpleData = ((\"James\", \"Sales\", 3000),\n(\"Michael\", \"Sales\", 4600),\n(\"Robert\", \"Sales\", 4100),\n(\"Maria\", \"Finance\", 3000),\n(\"James\", \"Sales\", 3000),\n(\"Scott\", \"Finance\", 3300),\n(\"Jen\", \"Finance\", 3900),\n(\"Jeff\", \"Marketing\", 3000),\n(\"Kumar\", \"Marketing\", 2000),\n(\"Saif\", \"Sales\", 4100)\n)\nrdd1 = sc.parallelize(simpleData)\ndf = rdd1.toDF([\"employee_name\",\"department\",\"salary\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3045e0f9-20a2-43d7-af47-f72f2a2fa29f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task_3","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
